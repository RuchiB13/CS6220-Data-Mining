{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WIBFfBu9tXc"
      },
      "source": [
        "# Part 1 - Apriori "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxbsPk169tXf"
      },
      "source": [
        "We begin by including the functions to generate frequent itemsets (via the Apriori algorithm) and resulting association rules:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv"
      ],
      "metadata": {
        "id": "9UEd-6E-AT8f"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "j1D3xzBh9tXg"
      },
      "outputs": [],
      "source": [
        "# (c) 2016 Everaldo Aguiar & Reid Johnson\n",
        "#\n",
        "# Modified from:\n",
        "# Marcel Caraciolo (https://gist.github.com/marcelcaraciolo/1423287)\n",
        "#\n",
        "# Functions to compute and extract association rules from a given frequent itemset \n",
        "# generated by the Apriori algorithm.\n",
        "#\n",
        "# The Apriori algorithm is defined by Agrawal and Srikant in:\n",
        "# Fast algorithms for mining association rules\n",
        "# Proc. 20th int. conf. very large data bases, VLDB. Vol. 1215. 1994\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "def load_dataset(filename):\n",
        "    '''Loads an example of market basket transactions from a provided csv file.\n",
        "\n",
        "    Returns: A list (database) of lists (transactions). Each element of a transaction is \n",
        "    an item.\n",
        "    '''\n",
        "\n",
        "    with open(filename,'r') as dest_f:\n",
        "        data_iter = csv.reader(dest_f, delimiter = ',', quotechar = '\"')\n",
        "        data = [data for data in data_iter]\n",
        "        data_array = np.asarray(data)\n",
        "        \n",
        "    return data_array\n",
        "\n",
        "def apriori(dataset, min_support=0.5, verbose=False):\n",
        "    \"\"\"Implements the Apriori algorithm.\n",
        "\n",
        "    The Apriori algorithm will iteratively generate new candidate \n",
        "    k-itemsets using the frequent (k-1)-itemsets found in the previous \n",
        "    iteration.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset : list\n",
        "        The dataset (a list of transactions) from which to generate \n",
        "        candidate itemsets.\n",
        "\n",
        "    min_support : float\n",
        "        The minimum support threshold. Defaults to 0.5.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    F : list\n",
        "        The list of frequent itemsets.\n",
        "\n",
        "    support_data : dict\n",
        "        The support data for all candidate itemsets.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] R. Agrawal, R. Srikant, \"Fast Algorithms for Mining Association \n",
        "           Rules\", 1994.\n",
        "\n",
        "    \"\"\"\n",
        "    C1 = create_candidates(dataset)\n",
        "    D = list(map(set, dataset))\n",
        "    F1, support_data = support_prune(D, C1, min_support, verbose=False) # prune candidate 1-itemsets\n",
        "    F = [F1] # list of frequent itemsets; initialized to frequent 1-itemsets\n",
        "    k = 2 # the itemset cardinality\n",
        "    while (len(F[k - 2]) > 0):\n",
        "        Ck = apriori_gen(F[k-2], k) # generate candidate itemsets\n",
        "        Fk, supK = support_prune(D, Ck, min_support) # prune candidate itemsets\n",
        "        support_data.update(supK) # update the support counts to reflect pruning\n",
        "        F.append(Fk) # add the pruned candidate itemsets to the list of frequent itemsets\n",
        "        k += 1\n",
        "\n",
        "    if verbose:\n",
        "        # Print a list of all the frequent itemsets.\n",
        "        for kset in F:\n",
        "            for item in kset:\n",
        "                print(\"\" \\\n",
        "                    + \"{\" \\\n",
        "                    + \"\".join(str(i) + \", \" for i in iter(item)).rstrip(', ') \\\n",
        "                    + \"}\" \\\n",
        "                    + \":  sup = \" + str(round(support_data[item], 3)))\n",
        "\n",
        "    return F, support_data\n",
        "\n",
        "def create_candidates(dataset, verbose=False):\n",
        "    \"\"\"Creates a list of candidate 1-itemsets from a list of transactions.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset : list\n",
        "        The dataset (a list of transactions) from which to generate candidate \n",
        "        itemsets.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    The list of candidate itemsets (c1) passed as a frozenset (a set that is \n",
        "    immutable and hashable).\n",
        "    \"\"\"\n",
        "    c1 = [] # list of all items in the database of transactions\n",
        "    for transaction in dataset:\n",
        "        for item in transaction:\n",
        "            if not [item] in c1:\n",
        "                c1.append([item])\n",
        "    c1.sort()\n",
        "\n",
        "    if verbose:\n",
        "        # Print a list of all the candidate items.\n",
        "        print(\"\" \\\n",
        "            + \"{\" \\\n",
        "            + \"\".join(str(i[0]) + \", \" for i in iter(c1)).rstrip(', ') \\\n",
        "            + \"}\")\n",
        "\n",
        "    # Map c1 to a frozenset because it will be the key of a dictionary.\n",
        "    return list(map(frozenset, c1))\n",
        "\n",
        "def support_prune(dataset, candidates, min_support, verbose=False):\n",
        "    \"\"\"Returns all candidate itemsets that meet a minimum support threshold.\n",
        "\n",
        "    By the apriori principle, if an itemset is frequent, then all of its \n",
        "    subsets must also be frequent. As a result, we can perform support-based \n",
        "    pruning to systematically control the exponential growth of candidate \n",
        "    itemsets. Thus, itemsets that do not meet the minimum support level are \n",
        "    pruned from the input list of itemsets (dataset).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset : list\n",
        "        The dataset (a list of transactions) from which to generate candidate \n",
        "        itemsets.\n",
        "\n",
        "    candidates : frozenset\n",
        "        The list of candidate itemsets.\n",
        "\n",
        "    min_support : float\n",
        "        The minimum support threshold.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    retlist : list\n",
        "        The list of frequent itemsets.\n",
        "\n",
        "    support_data : dict\n",
        "        The support data for all candidate itemsets.\n",
        "    \"\"\"\n",
        "    sscnt = {} # set for support counts\n",
        "    for tid in dataset:\n",
        "        for can in candidates:\n",
        "            if can.issubset(tid):\n",
        "                sscnt.setdefault(can, 0)\n",
        "                sscnt[can] += 1\n",
        "\n",
        "    num_items = float(len(dataset)) # total number of transactions in the dataset\n",
        "    retlist = [] # array for unpruned itemsets\n",
        "    support_data = {} # set for support data for corresponding itemsets\n",
        "    for key in sscnt:\n",
        "        # Calculate the support of itemset key.\n",
        "        support = sscnt[key] / num_items\n",
        "        if support >= min_support:\n",
        "            retlist.insert(0, key)\n",
        "        support_data[key] = support\n",
        "\n",
        "    # Print a list of the pruned itemsets.\n",
        "    if verbose:\n",
        "        for kset in retlist:\n",
        "            for item in kset:\n",
        "                print(\"{\" + str(item) + \"}\")\n",
        "        print(\"\")\n",
        "        for key in sscnt:\n",
        "            print(\"\" \\\n",
        "                + \"{\" \\\n",
        "                + \"\".join([str(i) + \", \" for i in iter(key)]).rstrip(', ') \\\n",
        "                + \"}\" \\\n",
        "                + \":  sup = \" + str(support_data[key]))\n",
        "\n",
        "    return retlist, support_data\n",
        "\n",
        "def apriori_gen(freq_sets, k):\n",
        "    \"\"\"Generates candidate itemsets (via the F_k-1 x F_k-1 method).\n",
        "\n",
        "    This operation generates new candidate k-itemsets based on the frequent \n",
        "    (k-1)-itemsets found in the previous iteration. The candidate generation \n",
        "    procedure merges a pair of frequent (k-1)-itemsets only if their first k-2 \n",
        "    items are identical.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    freq_sets : list\n",
        "        The list of frequent (k-1)-itemsets.\n",
        "\n",
        "    k : integer\n",
        "        The cardinality of the current itemsets being evaluated.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    retlist : list\n",
        "        The list of merged frequent itemsets.\n",
        "    \"\"\"\n",
        "    retList = [] # list of merged frequent itemsets\n",
        "    lenLk = len(freq_sets) # number of frequent itemsets\n",
        "    for i in range(lenLk):\n",
        "        for j in range(i+1, lenLk):\n",
        "            a=list(freq_sets[i])\n",
        "            b=list(freq_sets[j])\n",
        "            a.sort()\n",
        "            b.sort()\n",
        "            F1 = a[:k-2] # first k-2 items of freq_sets[i]\n",
        "            F2 = b[:k-2] # first k-2 items of freq_sets[j]\n",
        "\n",
        "            if F1 == F2: # if the first k-2 items are identical\n",
        "                # Merge the frequent itemsets.\n",
        "                retList.append(freq_sets[i] | freq_sets[j])\n",
        "\n",
        "    return retList\n",
        "\n",
        "def rules_from_conseq(freq_set, H, support_data, rules, min_confidence=0.5, verbose=False):\n",
        "    \"\"\"Generates a set of candidate rules.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    freq_set : frozenset\n",
        "        The complete list of frequent itemsets.\n",
        "\n",
        "    H : list\n",
        "        A list of frequent itemsets (of a particular length).\n",
        "\n",
        "    support_data : dict\n",
        "        The support data for all candidate itemsets.\n",
        "\n",
        "    rules : list\n",
        "        A potentially incomplete set of candidate rules above the minimum \n",
        "        confidence threshold.\n",
        "\n",
        "    min_confidence : float\n",
        "        The minimum confidence threshold. Defaults to 0.5.\n",
        "    \"\"\"\n",
        "    m = len(H[0])\n",
        "    if m == 1:\n",
        "        Hmp1 = calc_confidence(freq_set, H, support_data, rules, min_confidence, verbose)\n",
        "    if (len(freq_set) > (m+1)):\n",
        "        Hmp1 = apriori_gen(H, m+1) # generate candidate itemsets\n",
        "        Hmp1 = calc_confidence(freq_set, Hmp1, support_data, rules, min_confidence, verbose)\n",
        "        if len(Hmp1) > 1:\n",
        "            # If there are candidate rules above the minimum confidence \n",
        "            # threshold, recurse on the list of these candidate rules.\n",
        "            rules_from_conseq(freq_set, Hmp1, support_data, rules, min_confidence, verbose)\n",
        "\n",
        "def calc_confidence(freq_set, H, support_data, rules, min_confidence=0.5, verbose=False):\n",
        "    \"\"\"Evaluates the generated rules.\n",
        "\n",
        "    One measurement for quantifying the goodness of association rules is \n",
        "    confidence. The confidence for a rule 'P implies H' (P -> H) is defined as \n",
        "    the support for P and H divided by the support for P \n",
        "    (support (P|H) / support(P)), where the | symbol denotes the set union \n",
        "    (thus P|H means all the items in set P or in set H).\n",
        "\n",
        "    To calculate the confidence, we iterate through the frequent itemsets and \n",
        "    associated support data. For each frequent itemset, we divide the support \n",
        "    of the itemset by the support of the antecedent (left-hand-side of the \n",
        "    rule).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    freq_set : frozenset\n",
        "        The complete list of frequent itemsets.\n",
        "\n",
        "    H : list\n",
        "        A list of frequent itemsets (of a particular length).\n",
        "\n",
        "    min_support : float\n",
        "        The minimum support threshold.\n",
        "\n",
        "    rules : list\n",
        "        A potentially incomplete set of candidate rules above the minimum \n",
        "        confidence threshold.\n",
        "\n",
        "    min_confidence : float\n",
        "        The minimum confidence threshold. Defaults to 0.5.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pruned_H : list\n",
        "        The list of candidate rules above the minimum confidence threshold.\n",
        "    \"\"\"\n",
        "    pruned_H = [] # list of candidate rules above the minimum confidence threshold\n",
        "    for conseq in H: # iterate over the frequent itemsets\n",
        "        conf = support_data[freq_set] / support_data[freq_set - conseq]\n",
        "        if conf >= min_confidence:\n",
        "            rules.append((freq_set - conseq, conseq, conf))\n",
        "            pruned_H.append(conseq)\n",
        "\n",
        "            if verbose:\n",
        "                print(\"\" \\\n",
        "                    + \"{\" \\\n",
        "                    + \"\".join([str(i) + \", \" for i in iter(freq_set-conseq)]).rstrip(', ') \\\n",
        "                    + \"}\" \\\n",
        "                    + \" ---> \" \\\n",
        "                    + \"{\" \\\n",
        "                    + \"\".join([str(i) + \", \" for i in iter(conseq)]).rstrip(', ') \\\n",
        "                    + \"}\" \\\n",
        "                    + \":  conf = \" + str(round(conf, 3)) \\\n",
        "                    + \", sup = \" + str(round(support_data[freq_set], 3)))\n",
        "\n",
        "    return pruned_H\n",
        "\n",
        "def generate_rules(F, support_data, min_confidence=0.5, verbose=True):\n",
        "    \"\"\"Generates a set of candidate rules from a list of frequent itemsets.\n",
        "\n",
        "    For each frequent itemset, we calculate the confidence of using a\n",
        "    particular item as the rule consequent (right-hand-side of the rule). By \n",
        "    testing and merging the remaining rules, we recursively create a list of \n",
        "    pruned rules.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    F : list\n",
        "        A list of frequent itemsets.\n",
        "\n",
        "    support_data : dict\n",
        "        The corresponding support data for the frequent itemsets (L).\n",
        "\n",
        "    min_confidence : float\n",
        "        The minimum confidence threshold. Defaults to 0.5.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    rules : list\n",
        "        The list of candidate rules above the minimum confidence threshold.\n",
        "    \"\"\"\n",
        "    rules = []\n",
        "    for i in range(1, len(F)):\n",
        "        for freq_set in F[i]:\n",
        "            H1 = [frozenset([itemset]) for itemset in freq_set]\n",
        "            if (i > 1):\n",
        "                rules_from_conseq(freq_set, H1, support_data, rules, min_confidence, verbose)\n",
        "            else:\n",
        "                calc_confidence(freq_set, H1, support_data, rules, min_confidence, verbose)\n",
        "\n",
        "    return rules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3k99tonl9tXl"
      },
      "source": [
        "### To load our dataset of grocery transactions, use the command below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3YITqS-9tXm",
        "outputId": "b5899ccd-62eb-47b3-affe-21974dae2878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset('/content/grocery.csv')\n",
        "D = list(map(set, dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQQ0k-Ff9tXn"
      },
      "source": [
        "### _dataset_ is now a ndarray containing each of the 9835 transactions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3THbJS9K9tXn",
        "outputId": "1fedfd6e-eb73-443d-b092-95f54d794e20"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "type(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIkzPv6_9tXp",
        "outputId": "dc900787-a8ae-41ff-b561-b03bfeea2324"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9835,)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XU8Ynrav9tXq",
        "outputId": "3ff7d12e-e6cc-4097-d032-1d0436b6ca4e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['citrus fruit', 'semi-finished bread', 'margarine', 'ready soups']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWBzTNe_9tXr",
        "outputId": "d03a79fb-1a18-42b2-8228-d3cb5c0f6557"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tropical fruit', 'yogurt', 'coffee']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "dataset[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3lPta189tXs"
      },
      "source": [
        "### _D_ Contains that dataset in a set format (which excludes duplicated items and sorts them)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYXM-yWZ9tXs",
        "outputId": "f0038b57-aa36-4eb7-dca0-9bf7bc908d4e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "set"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "type(D[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZGlyyuV9tXs",
        "outputId": "7f24615d-2080-47c6-aad5-2e4a747785ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'citrus fruit', 'margarine', 'ready soups', 'semi-finished bread'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "D[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TASK 1"
      ],
      "metadata": {
        "id": "r34vjDZCApHg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-Pg2llF9tXt",
        "outputId": "1d86acd4-6a5e-4f20-ce82-c8bee05e8914"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{Instant food products, UHT-milk, abrasive cleaner, artif. sweetener, baby cosmetics, baby food, bags, baking powder, bathroom cleaner, beef, berries, beverages, bottled beer, bottled water, brandy, brown bread, butter, butter milk, cake bar, candles, candy, canned beer, canned fish, canned fruit, canned vegetables, cat food, cereals, chewing gum, chicken, chocolate, chocolate marshmallow, citrus fruit, cleaner, cling film/bags, cocoa drinks, coffee, condensed milk, cooking chocolate, cookware, cream, cream cheese , curd, curd cheese, decalcifier, dental care, dessert, detergent, dish cleaner, dishes, dog food, domestic eggs, female sanitary products, finished products, fish, flour, flower (seeds), flower soil/fertilizer, frankfurter, frozen chicken, frozen dessert, frozen fish, frozen fruits, frozen meals, frozen potato products, frozen vegetables, fruit/vegetable juice, grapes, hair spray, ham, hamburger meat, hard cheese, herbs, honey, house keeping products, hygiene articles, ice cream, instant coffee, jam, ketchup, kitchen towels, kitchen utensil, light bulbs, liqueur, liquor, liquor (appetizer), liver loaf, long life bakery product, make up remover, male cosmetics, margarine, mayonnaise, meat, meat spreads, misc. beverages, mustard, napkins, newspapers, nut snack, nuts/prunes, oil, onions, organic products, organic sausage, other vegetables, packaged fruit/vegetables, pasta, pastry, pet care, photo/film, pickled vegetables, pip fruit, popcorn, pork, pot plants, potato products, preservation products, processed cheese, prosecco, pudding powder, ready soups, red/blush wine, rice, roll products , rolls/buns, root vegetables, rubbing alcohol, rum, salad dressing, salt, salty snack, sauces, sausage, seasonal products, semi-finished bread, shopping bags, skin care, sliced cheese, snack products, soap, soda, soft cheese, softener, sound storage medium, soups, sparkling wine, specialty bar, specialty cheese, specialty chocolate, specialty fat, specialty vegetables, spices, spread cheese, sugar, sweet spreads, syrup, tea, tidbits, toilet cleaner, tropical fruit, turkey, vinegar, waffles, whipped/sour cream, whisky, white bread, white wine, whole milk, yogurt, zwieback}\n"
          ]
        }
      ],
      "source": [
        "# Generate candidate itemsets.\n",
        "C1 = create_candidates(dataset, verbose=True) # candidate 1-itemsets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prune candidate 1-itemsets via support-based pruning to generate frequent 1-itemsets.\n",
        "F1, support_data = support_prune(dataset, C1, 0.1, verbose=True)"
      ],
      "metadata": {
        "id": "GPKP4vV4OiBC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b9e6895-1198-4e69-8653-46ea68757fd8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{root vegetables}\n",
            "{soda}\n",
            "{bottled water}\n",
            "{rolls/buns}\n",
            "{other vegetables}\n",
            "{whole milk}\n",
            "{yogurt}\n",
            "{tropical fruit}\n",
            "\n",
            "{citrus fruit}:  sup = 0.08276563294356888\n",
            "{margarine}:  sup = 0.05856634468734113\n",
            "{ready soups}:  sup = 0.0018301982714794102\n",
            "{semi-finished bread}:  sup = 0.017691916624300967\n",
            "{coffee}:  sup = 0.05805795627859685\n",
            "{tropical fruit}:  sup = 0.10493136756481952\n",
            "{yogurt}:  sup = 0.13950177935943062\n",
            "{whole milk}:  sup = 0.25551601423487547\n",
            "{cream cheese}:  sup = 0.03965429588205389\n",
            "{meat spreads}:  sup = 0.004270462633451958\n",
            "{pip fruit}:  sup = 0.07564819522114896\n",
            "{condensed milk}:  sup = 0.010269445856634469\n",
            "{long life bakery product}:  sup = 0.037417386883579054\n",
            "{other vegetables}:  sup = 0.1934926283680732\n",
            "{abrasive cleaner}:  sup = 0.0035587188612099642\n",
            "{butter}:  sup = 0.05541433655312659\n",
            "{rice}:  sup = 0.007625826131164209\n",
            "{rolls/buns}:  sup = 0.18393492628368074\n",
            "{UHT-milk}:  sup = 0.03345195729537367\n",
            "{bottled beer}:  sup = 0.08052872394509406\n",
            "{liquor (appetizer)}:  sup = 0.007930859176410779\n",
            "{pot plants}:  sup = 0.01728520589730554\n",
            "{cereals}:  sup = 0.0056939501779359435\n",
            "{bottled water}:  sup = 0.11052364006100661\n",
            "{chocolate}:  sup = 0.04961870869344179\n",
            "{white bread}:  sup = 0.042094560244026434\n",
            "{curd}:  sup = 0.05327910523640061\n",
            "{dishes}:  sup = 0.01759023894255211\n",
            "{flour}:  sup = 0.017386883579054397\n",
            "{beef}:  sup = 0.05246568378240976\n",
            "{frankfurter}:  sup = 0.058973055414336555\n",
            "{soda}:  sup = 0.17437722419928825\n",
            "{chicken}:  sup = 0.04290798169801729\n",
            "{fruit/vegetable juice}:  sup = 0.0722928317234367\n",
            "{newspapers}:  sup = 0.07981698017285206\n",
            "{sugar}:  sup = 0.03385866802236909\n",
            "{packaged fruit/vegetables}:  sup = 0.013014743263853584\n",
            "{specialty bar}:  sup = 0.027351296390442297\n",
            "{butter milk}:  sup = 0.027961362480935434\n",
            "{pastry}:  sup = 0.08896797153024912\n",
            "{detergent}:  sup = 0.019217081850533807\n",
            "{processed cheese}:  sup = 0.016573462125063547\n",
            "{bathroom cleaner}:  sup = 0.0027452974072191155\n",
            "{candy}:  sup = 0.0298932384341637\n",
            "{frozen dessert}:  sup = 0.010777834265378749\n",
            "{root vegetables}:  sup = 0.10899847483477376\n",
            "{salty snack}:  sup = 0.03782409761057448\n",
            "{sweet spreads}:  sup = 0.009049313675648195\n",
            "{waffles}:  sup = 0.038434163701067614\n",
            "{canned beer}:  sup = 0.07768174885612608\n",
            "{sausage}:  sup = 0.09395017793594305\n",
            "{brown bread}:  sup = 0.06487036095577021\n",
            "{shopping bags}:  sup = 0.09852567361464158\n",
            "{beverages}:  sup = 0.026029486527707167\n",
            "{hamburger meat}:  sup = 0.033248601931875954\n",
            "{hygiene articles}:  sup = 0.03294356888662939\n",
            "{napkins}:  sup = 0.05236400610066091\n",
            "{spices}:  sup = 0.005185561769191663\n",
            "{artif. sweetener}:  sup = 0.003253685815963396\n",
            "{berries}:  sup = 0.033248601931875954\n",
            "{pork}:  sup = 0.05765124555160142\n",
            "{whipped/sour cream}:  sup = 0.07168276563294357\n",
            "{grapes}:  sup = 0.022369089984748347\n",
            "{dessert}:  sup = 0.03711235383833249\n",
            "{zwieback}:  sup = 0.006914082358922217\n",
            "{domestic eggs}:  sup = 0.06344687341128623\n",
            "{spread cheese}:  sup = 0.011184544992374174\n",
            "{misc. beverages}:  sup = 0.02836807320793086\n",
            "{hard cheese}:  sup = 0.024504321301474327\n",
            "{cat food}:  sup = 0.023284189120488054\n",
            "{ham}:  sup = 0.026029486527707167\n",
            "{baking powder}:  sup = 0.017691916624300967\n",
            "{turkey}:  sup = 0.00813421453990849\n",
            "{pickled vegetables}:  sup = 0.017895271987798677\n",
            "{chewing gum}:  sup = 0.021047280122013217\n",
            "{chocolate marshmallow}:  sup = 0.009049313675648195\n",
            "{oil}:  sup = 0.02806304016268429\n",
            "{ice cream}:  sup = 0.025012709710218607\n",
            "{canned fish}:  sup = 0.015048296898830707\n",
            "{frozen vegetables}:  sup = 0.04809354346720895\n",
            "{seasonal products}:  sup = 0.014234875444839857\n",
            "{curd cheese}:  sup = 0.005083884087442806\n",
            "{red/blush wine}:  sup = 0.019217081850533807\n",
            "{frozen potato products}:  sup = 0.008439247585155059\n",
            "{candles}:  sup = 0.008947635993899338\n",
            "{flower (seeds)}:  sup = 0.010371123538383325\n",
            "{specialty chocolate}:  sup = 0.03040162684290798\n",
            "{specialty fat}:  sup = 0.0036603965429588205\n",
            "{sparkling wine}:  sup = 0.005592272496187087\n",
            "{salt}:  sup = 0.010777834265378749\n",
            "{frozen meals}:  sup = 0.02836807320793086\n",
            "{canned vegetables}:  sup = 0.010777834265378749\n",
            "{onions}:  sup = 0.031011692933401117\n",
            "{herbs}:  sup = 0.01626842907981698\n",
            "{white wine}:  sup = 0.019013726487036097\n",
            "{brandy}:  sup = 0.004168784951703101\n",
            "{photo/film}:  sup = 0.009252669039145907\n",
            "{sliced cheese}:  sup = 0.024504321301474327\n",
            "{pasta}:  sup = 0.015048296898830707\n",
            "{softener}:  sup = 0.005490594814438231\n",
            "{cling film/bags}:  sup = 0.011387900355871887\n",
            "{fish}:  sup = 0.0029486527707168276\n",
            "{male cosmetics}:  sup = 0.004575495678698526\n",
            "{canned fruit}:  sup = 0.003253685815963396\n",
            "{Instant food products}:  sup = 0.008032536858159633\n",
            "{soft cheese}:  sup = 0.01708185053380783\n",
            "{honey}:  sup = 0.001525165226232842\n",
            "{dental care}:  sup = 0.005795627859684799\n",
            "{popcorn}:  sup = 0.007219115404168785\n",
            "{cake bar}:  sup = 0.013218098627351297\n",
            "{snack products}:  sup = 0.003050330452465684\n",
            "{flower soil/fertilizer}:  sup = 0.0019318759532282665\n",
            "{specialty cheese}:  sup = 0.008540925266903915\n",
            "{finished products}:  sup = 0.006507371631926792\n",
            "{cocoa drinks}:  sup = 0.0022369089984748346\n",
            "{dog food}:  sup = 0.008540925266903915\n",
            "{prosecco}:  sup = 0.0020335536349771225\n",
            "{frozen fish}:  sup = 0.011692933401118455\n",
            "{make up remover}:  sup = 0.000813421453990849\n",
            "{cleaner}:  sup = 0.005083884087442806\n",
            "{female sanitary products}:  sup = 0.006100660904931368\n",
            "{cookware}:  sup = 0.0027452974072191155\n",
            "{dish cleaner}:  sup = 0.01047280122013218\n",
            "{meat}:  sup = 0.025826131164209457\n",
            "{tea}:  sup = 0.003863751906456533\n",
            "{mustard}:  sup = 0.011997966446365024\n",
            "{house keeping products}:  sup = 0.008337569903406202\n",
            "{skin care}:  sup = 0.0035587188612099642\n",
            "{potato products}:  sup = 0.0028469750889679717\n",
            "{liquor}:  sup = 0.011082867310625319\n",
            "{pet care}:  sup = 0.00945602440264362\n",
            "{soups}:  sup = 0.00681240467717336\n",
            "{rum}:  sup = 0.004473817996949669\n",
            "{salad dressing}:  sup = 0.000813421453990849\n",
            "{sauces}:  sup = 0.005490594814438231\n",
            "{vinegar}:  sup = 0.006507371631926792\n",
            "{soap}:  sup = 0.0026436197254702592\n",
            "{hair spray}:  sup = 0.0011184544992374173\n",
            "{instant coffee}:  sup = 0.007422470767666497\n",
            "{roll products}:  sup = 0.010269445856634469\n",
            "{mayonnaise}:  sup = 0.009150991357397052\n",
            "{rubbing alcohol}:  sup = 0.0010167768174885613\n",
            "{syrup}:  sup = 0.003253685815963396\n",
            "{liver loaf}:  sup = 0.005083884087442806\n",
            "{baby cosmetics}:  sup = 0.0006100660904931368\n",
            "{organic products}:  sup = 0.001626842907981698\n",
            "{nut snack}:  sup = 0.00315200813421454\n",
            "{kitchen towels}:  sup = 0.005998983223182512\n",
            "{frozen chicken}:  sup = 0.0006100660904931368\n",
            "{light bulbs}:  sup = 0.004168784951703101\n",
            "{ketchup}:  sup = 0.004270462633451958\n",
            "{jam}:  sup = 0.005388917132689374\n",
            "{decalcifier}:  sup = 0.001525165226232842\n",
            "{nuts/prunes}:  sup = 0.003355363497712252\n",
            "{liqueur}:  sup = 0.0009150991357397051\n",
            "{organic sausage}:  sup = 0.0022369089984748346\n",
            "{cream}:  sup = 0.0013218098627351296\n",
            "{toilet cleaner}:  sup = 0.0007117437722419929\n",
            "{specialty vegetables}:  sup = 0.0017285205897305542\n",
            "{baby food}:  sup = 0.00010167768174885612\n",
            "{pudding powder}:  sup = 0.002338586680223691\n",
            "{tidbits}:  sup = 0.002338586680223691\n",
            "{whisky}:  sup = 0.000813421453990849\n",
            "{frozen fruits}:  sup = 0.0012201321809862736\n",
            "{bags}:  sup = 0.0004067107269954245\n",
            "{cooking chocolate}:  sup = 0.002541942043721403\n",
            "{sound storage medium}:  sup = 0.00010167768174885612\n",
            "{kitchen utensil}:  sup = 0.0004067107269954245\n",
            "{preservation products}:  sup = 0.00020335536349771224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate all the frequent itemsets using the Apriori algorithm.\n",
        "F, support_dataa = apriori(dataset, min_support=0.02, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Xpnz5GAZzhS",
        "outputId": "4785691b-d991-4e48-daaf-ac6d85375f60"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{meat}:  sup = 0.026\n",
            "{sliced cheese}:  sup = 0.025\n",
            "{onions}:  sup = 0.031\n",
            "{frozen meals}:  sup = 0.028\n",
            "{specialty chocolate}:  sup = 0.03\n",
            "{frozen vegetables}:  sup = 0.048\n",
            "{ice cream}:  sup = 0.025\n",
            "{oil}:  sup = 0.028\n",
            "{chewing gum}:  sup = 0.021\n",
            "{ham}:  sup = 0.026\n",
            "{cat food}:  sup = 0.023\n",
            "{hard cheese}:  sup = 0.025\n",
            "{misc. beverages}:  sup = 0.028\n",
            "{domestic eggs}:  sup = 0.063\n",
            "{dessert}:  sup = 0.037\n",
            "{grapes}:  sup = 0.022\n",
            "{whipped/sour cream}:  sup = 0.072\n",
            "{pork}:  sup = 0.058\n",
            "{berries}:  sup = 0.033\n",
            "{napkins}:  sup = 0.052\n",
            "{hygiene articles}:  sup = 0.033\n",
            "{hamburger meat}:  sup = 0.033\n",
            "{beverages}:  sup = 0.026\n",
            "{shopping bags}:  sup = 0.099\n",
            "{brown bread}:  sup = 0.065\n",
            "{sausage}:  sup = 0.094\n",
            "{canned beer}:  sup = 0.078\n",
            "{waffles}:  sup = 0.038\n",
            "{salty snack}:  sup = 0.038\n",
            "{root vegetables}:  sup = 0.109\n",
            "{candy}:  sup = 0.03\n",
            "{pastry}:  sup = 0.089\n",
            "{butter milk}:  sup = 0.028\n",
            "{specialty bar}:  sup = 0.027\n",
            "{sugar}:  sup = 0.034\n",
            "{newspapers}:  sup = 0.08\n",
            "{fruit/vegetable juice}:  sup = 0.072\n",
            "{chicken}:  sup = 0.043\n",
            "{soda}:  sup = 0.174\n",
            "{frankfurter}:  sup = 0.059\n",
            "{beef}:  sup = 0.052\n",
            "{curd}:  sup = 0.053\n",
            "{white bread}:  sup = 0.042\n",
            "{chocolate}:  sup = 0.05\n",
            "{bottled water}:  sup = 0.111\n",
            "{bottled beer}:  sup = 0.081\n",
            "{UHT-milk}:  sup = 0.033\n",
            "{rolls/buns}:  sup = 0.184\n",
            "{butter}:  sup = 0.055\n",
            "{other vegetables}:  sup = 0.193\n",
            "{long life bakery product}:  sup = 0.037\n",
            "{pip fruit}:  sup = 0.076\n",
            "{cream cheese}:  sup = 0.04\n",
            "{whole milk}:  sup = 0.256\n",
            "{yogurt}:  sup = 0.14\n",
            "{tropical fruit}:  sup = 0.105\n",
            "{coffee}:  sup = 0.058\n",
            "{margarine}:  sup = 0.059\n",
            "{citrus fruit}:  sup = 0.083\n",
            "{whipped/sour cream, yogurt}:  sup = 0.021\n",
            "{yogurt, other vegetables}:  sup = 0.043\n",
            "{pip fruit, other vegetables}:  sup = 0.026\n",
            "{other vegetables, pastry}:  sup = 0.023\n",
            "{shopping bags, other vegetables}:  sup = 0.023\n",
            "{other vegetables, sausage}:  sup = 0.027\n",
            "{whole milk, bottled beer}:  sup = 0.02\n",
            "{shopping bags, whole milk}:  sup = 0.025\n",
            "{citrus fruit, other vegetables}:  sup = 0.029\n",
            "{fruit/vegetable juice, whole milk}:  sup = 0.027\n",
            "{whole milk, frankfurter}:  sup = 0.021\n",
            "{whole milk, newspapers}:  sup = 0.027\n",
            "{whole milk, margarine}:  sup = 0.024\n",
            "{pip fruit, tropical fruit}:  sup = 0.02\n",
            "{whole milk, pip fruit}:  sup = 0.03\n",
            "{whole milk, rolls/buns}:  sup = 0.057\n",
            "{whole milk, beef}:  sup = 0.021\n",
            "{whole milk, sausage}:  sup = 0.03\n",
            "{whole milk, frozen vegetables}:  sup = 0.02\n",
            "{rolls/buns, pastry}:  sup = 0.021\n",
            "{fruit/vegetable juice, other vegetables}:  sup = 0.021\n",
            "{domestic eggs, other vegetables}:  sup = 0.022\n",
            "{butter, other vegetables}:  sup = 0.02\n",
            "{yogurt, rolls/buns}:  sup = 0.034\n",
            "{bottled water, soda}:  sup = 0.029\n",
            "{tropical fruit, soda}:  sup = 0.021\n",
            "{yogurt, soda}:  sup = 0.027\n",
            "{whole milk, pastry}:  sup = 0.033\n",
            "{yogurt, root vegetables}:  sup = 0.026\n",
            "{whole milk, brown bread}:  sup = 0.025\n",
            "{whole milk, domestic eggs}:  sup = 0.03\n",
            "{pastry, soda}:  sup = 0.021\n",
            "{whole milk, soda}:  sup = 0.04\n",
            "{other vegetables, soda}:  sup = 0.033\n",
            "{whole milk, pork}:  sup = 0.022\n",
            "{pork, other vegetables}:  sup = 0.022\n",
            "{whole milk, whipped/sour cream}:  sup = 0.032\n",
            "{whipped/sour cream, other vegetables}:  sup = 0.029\n",
            "{whole milk, root vegetables}:  sup = 0.049\n",
            "{rolls/buns, bottled water}:  sup = 0.024\n",
            "{shopping bags, soda}:  sup = 0.025\n",
            "{rolls/buns, sausage}:  sup = 0.031\n",
            "{sausage, soda}:  sup = 0.024\n",
            "{rolls/buns, tropical fruit}:  sup = 0.025\n",
            "{root vegetables, tropical fruit}:  sup = 0.021\n",
            "{other vegetables, root vegetables}:  sup = 0.047\n",
            "{rolls/buns, root vegetables}:  sup = 0.024\n",
            "{rolls/buns, soda}:  sup = 0.038\n",
            "{yogurt, citrus fruit}:  sup = 0.022\n",
            "{whole milk, citrus fruit}:  sup = 0.031\n",
            "{whole milk, tropical fruit}:  sup = 0.042\n",
            "{yogurt, bottled water}:  sup = 0.023\n",
            "{whole milk, bottled water}:  sup = 0.034\n",
            "{whole milk, curd}:  sup = 0.026\n",
            "{other vegetables, tropical fruit}:  sup = 0.036\n",
            "{other vegetables, bottled water}:  sup = 0.025\n",
            "{other vegetables, rolls/buns}:  sup = 0.043\n",
            "{whole milk, yogurt}:  sup = 0.056\n",
            "{whole milk, butter}:  sup = 0.028\n",
            "{whole milk, other vegetables}:  sup = 0.075\n",
            "{yogurt, tropical fruit}:  sup = 0.029\n",
            "{whole milk, yogurt, other vegetables}:  sup = 0.022\n",
            "{whole milk, other vegetables, root vegetables}:  sup = 0.023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the association rules from a list of frequent itemsets.\n",
        "H = generate_rules(F, support_dataa, min_confidence=0.1, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMpl3N-6Zzpl",
        "outputId": "7e46ef84-050d-4221-cda2-0e66bac15805"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{yogurt} ---> {whipped/sour cream}:  conf = 0.149, sup = 0.021\n",
            "{whipped/sour cream} ---> {yogurt}:  conf = 0.289, sup = 0.021\n",
            "{other vegetables} ---> {yogurt}:  conf = 0.224, sup = 0.043\n",
            "{yogurt} ---> {other vegetables}:  conf = 0.311, sup = 0.043\n",
            "{other vegetables} ---> {pip fruit}:  conf = 0.135, sup = 0.026\n",
            "{pip fruit} ---> {other vegetables}:  conf = 0.345, sup = 0.026\n",
            "{pastry} ---> {other vegetables}:  conf = 0.254, sup = 0.023\n",
            "{other vegetables} ---> {pastry}:  conf = 0.117, sup = 0.023\n",
            "{other vegetables} ---> {shopping bags}:  conf = 0.12, sup = 0.023\n",
            "{shopping bags} ---> {other vegetables}:  conf = 0.235, sup = 0.023\n",
            "{sausage} ---> {other vegetables}:  conf = 0.287, sup = 0.027\n",
            "{other vegetables} ---> {sausage}:  conf = 0.139, sup = 0.027\n",
            "{bottled beer} ---> {whole milk}:  conf = 0.254, sup = 0.02\n",
            "{shopping bags} ---> {whole milk}:  conf = 0.249, sup = 0.025\n",
            "{other vegetables} ---> {citrus fruit}:  conf = 0.149, sup = 0.029\n",
            "{citrus fruit} ---> {other vegetables}:  conf = 0.349, sup = 0.029\n",
            "{whole milk} ---> {fruit/vegetable juice}:  conf = 0.104, sup = 0.027\n",
            "{fruit/vegetable juice} ---> {whole milk}:  conf = 0.368, sup = 0.027\n",
            "{frankfurter} ---> {whole milk}:  conf = 0.348, sup = 0.021\n",
            "{newspapers} ---> {whole milk}:  conf = 0.343, sup = 0.027\n",
            "{whole milk} ---> {newspapers}:  conf = 0.107, sup = 0.027\n",
            "{margarine} ---> {whole milk}:  conf = 0.413, sup = 0.024\n",
            "{tropical fruit} ---> {pip fruit}:  conf = 0.195, sup = 0.02\n",
            "{pip fruit} ---> {tropical fruit}:  conf = 0.27, sup = 0.02\n",
            "{pip fruit} ---> {whole milk}:  conf = 0.398, sup = 0.03\n",
            "{whole milk} ---> {pip fruit}:  conf = 0.118, sup = 0.03\n",
            "{rolls/buns} ---> {whole milk}:  conf = 0.308, sup = 0.057\n",
            "{whole milk} ---> {rolls/buns}:  conf = 0.222, sup = 0.057\n",
            "{beef} ---> {whole milk}:  conf = 0.405, sup = 0.021\n",
            "{sausage} ---> {whole milk}:  conf = 0.318, sup = 0.03\n",
            "{whole milk} ---> {sausage}:  conf = 0.117, sup = 0.03\n",
            "{frozen vegetables} ---> {whole milk}:  conf = 0.425, sup = 0.02\n",
            "{pastry} ---> {rolls/buns}:  conf = 0.235, sup = 0.021\n",
            "{rolls/buns} ---> {pastry}:  conf = 0.114, sup = 0.021\n",
            "{other vegetables} ---> {fruit/vegetable juice}:  conf = 0.109, sup = 0.021\n",
            "{fruit/vegetable juice} ---> {other vegetables}:  conf = 0.291, sup = 0.021\n",
            "{other vegetables} ---> {domestic eggs}:  conf = 0.115, sup = 0.022\n",
            "{domestic eggs} ---> {other vegetables}:  conf = 0.351, sup = 0.022\n",
            "{other vegetables} ---> {butter}:  conf = 0.104, sup = 0.02\n",
            "{butter} ---> {other vegetables}:  conf = 0.361, sup = 0.02\n",
            "{rolls/buns} ---> {yogurt}:  conf = 0.187, sup = 0.034\n",
            "{yogurt} ---> {rolls/buns}:  conf = 0.246, sup = 0.034\n",
            "{soda} ---> {bottled water}:  conf = 0.166, sup = 0.029\n",
            "{bottled water} ---> {soda}:  conf = 0.262, sup = 0.029\n",
            "{soda} ---> {tropical fruit}:  conf = 0.12, sup = 0.021\n",
            "{tropical fruit} ---> {soda}:  conf = 0.199, sup = 0.021\n",
            "{soda} ---> {yogurt}:  conf = 0.157, sup = 0.027\n",
            "{yogurt} ---> {soda}:  conf = 0.196, sup = 0.027\n",
            "{pastry} ---> {whole milk}:  conf = 0.374, sup = 0.033\n",
            "{whole milk} ---> {pastry}:  conf = 0.13, sup = 0.033\n",
            "{root vegetables} ---> {yogurt}:  conf = 0.237, sup = 0.026\n",
            "{yogurt} ---> {root vegetables}:  conf = 0.185, sup = 0.026\n",
            "{brown bread} ---> {whole milk}:  conf = 0.389, sup = 0.025\n",
            "{domestic eggs} ---> {whole milk}:  conf = 0.473, sup = 0.03\n",
            "{whole milk} ---> {domestic eggs}:  conf = 0.117, sup = 0.03\n",
            "{soda} ---> {pastry}:  conf = 0.121, sup = 0.021\n",
            "{pastry} ---> {soda}:  conf = 0.237, sup = 0.021\n",
            "{soda} ---> {whole milk}:  conf = 0.23, sup = 0.04\n",
            "{whole milk} ---> {soda}:  conf = 0.157, sup = 0.04\n",
            "{soda} ---> {other vegetables}:  conf = 0.188, sup = 0.033\n",
            "{other vegetables} ---> {soda}:  conf = 0.169, sup = 0.033\n",
            "{pork} ---> {whole milk}:  conf = 0.384, sup = 0.022\n",
            "{other vegetables} ---> {pork}:  conf = 0.112, sup = 0.022\n",
            "{pork} ---> {other vegetables}:  conf = 0.376, sup = 0.022\n",
            "{whipped/sour cream} ---> {whole milk}:  conf = 0.45, sup = 0.032\n",
            "{whole milk} ---> {whipped/sour cream}:  conf = 0.126, sup = 0.032\n",
            "{other vegetables} ---> {whipped/sour cream}:  conf = 0.149, sup = 0.029\n",
            "{whipped/sour cream} ---> {other vegetables}:  conf = 0.403, sup = 0.029\n",
            "{root vegetables} ---> {whole milk}:  conf = 0.449, sup = 0.049\n",
            "{whole milk} ---> {root vegetables}:  conf = 0.191, sup = 0.049\n",
            "{bottled water} ---> {rolls/buns}:  conf = 0.219, sup = 0.024\n",
            "{rolls/buns} ---> {bottled water}:  conf = 0.132, sup = 0.024\n",
            "{soda} ---> {shopping bags}:  conf = 0.141, sup = 0.025\n",
            "{shopping bags} ---> {soda}:  conf = 0.25, sup = 0.025\n",
            "{sausage} ---> {rolls/buns}:  conf = 0.326, sup = 0.031\n",
            "{rolls/buns} ---> {sausage}:  conf = 0.166, sup = 0.031\n",
            "{soda} ---> {sausage}:  conf = 0.139, sup = 0.024\n",
            "{sausage} ---> {soda}:  conf = 0.259, sup = 0.024\n",
            "{tropical fruit} ---> {rolls/buns}:  conf = 0.234, sup = 0.025\n",
            "{rolls/buns} ---> {tropical fruit}:  conf = 0.134, sup = 0.025\n",
            "{tropical fruit} ---> {root vegetables}:  conf = 0.201, sup = 0.021\n",
            "{root vegetables} ---> {tropical fruit}:  conf = 0.193, sup = 0.021\n",
            "{root vegetables} ---> {other vegetables}:  conf = 0.435, sup = 0.047\n",
            "{other vegetables} ---> {root vegetables}:  conf = 0.245, sup = 0.047\n",
            "{root vegetables} ---> {rolls/buns}:  conf = 0.223, sup = 0.024\n",
            "{rolls/buns} ---> {root vegetables}:  conf = 0.132, sup = 0.024\n",
            "{soda} ---> {rolls/buns}:  conf = 0.22, sup = 0.038\n",
            "{rolls/buns} ---> {soda}:  conf = 0.208, sup = 0.038\n",
            "{citrus fruit} ---> {yogurt}:  conf = 0.262, sup = 0.022\n",
            "{yogurt} ---> {citrus fruit}:  conf = 0.155, sup = 0.022\n",
            "{citrus fruit} ---> {whole milk}:  conf = 0.369, sup = 0.031\n",
            "{whole milk} ---> {citrus fruit}:  conf = 0.119, sup = 0.031\n",
            "{tropical fruit} ---> {whole milk}:  conf = 0.403, sup = 0.042\n",
            "{whole milk} ---> {tropical fruit}:  conf = 0.166, sup = 0.042\n",
            "{bottled water} ---> {yogurt}:  conf = 0.208, sup = 0.023\n",
            "{yogurt} ---> {bottled water}:  conf = 0.165, sup = 0.023\n",
            "{bottled water} ---> {whole milk}:  conf = 0.311, sup = 0.034\n",
            "{whole milk} ---> {bottled water}:  conf = 0.135, sup = 0.034\n",
            "{curd} ---> {whole milk}:  conf = 0.49, sup = 0.026\n",
            "{whole milk} ---> {curd}:  conf = 0.102, sup = 0.026\n",
            "{tropical fruit} ---> {other vegetables}:  conf = 0.342, sup = 0.036\n",
            "{other vegetables} ---> {tropical fruit}:  conf = 0.185, sup = 0.036\n",
            "{bottled water} ---> {other vegetables}:  conf = 0.224, sup = 0.025\n",
            "{other vegetables} ---> {bottled water}:  conf = 0.128, sup = 0.025\n",
            "{rolls/buns} ---> {other vegetables}:  conf = 0.232, sup = 0.043\n",
            "{other vegetables} ---> {rolls/buns}:  conf = 0.22, sup = 0.043\n",
            "{yogurt} ---> {whole milk}:  conf = 0.402, sup = 0.056\n",
            "{whole milk} ---> {yogurt}:  conf = 0.219, sup = 0.056\n",
            "{butter} ---> {whole milk}:  conf = 0.497, sup = 0.028\n",
            "{whole milk} ---> {butter}:  conf = 0.108, sup = 0.028\n",
            "{other vegetables} ---> {whole milk}:  conf = 0.387, sup = 0.075\n",
            "{whole milk} ---> {other vegetables}:  conf = 0.293, sup = 0.075\n",
            "{tropical fruit} ---> {yogurt}:  conf = 0.279, sup = 0.029\n",
            "{yogurt} ---> {tropical fruit}:  conf = 0.21, sup = 0.029\n",
            "{yogurt, other vegetables} ---> {whole milk}:  conf = 0.513, sup = 0.022\n",
            "{whole milk, other vegetables} ---> {yogurt}:  conf = 0.298, sup = 0.022\n",
            "{whole milk, yogurt} ---> {other vegetables}:  conf = 0.397, sup = 0.022\n",
            "{other vegetables} ---> {whole milk, yogurt}:  conf = 0.115, sup = 0.022\n",
            "{yogurt} ---> {whole milk, other vegetables}:  conf = 0.16, sup = 0.022\n",
            "{other vegetables, root vegetables} ---> {whole milk}:  conf = 0.489, sup = 0.023\n",
            "{whole milk, root vegetables} ---> {other vegetables}:  conf = 0.474, sup = 0.023\n",
            "{whole milk, other vegetables} ---> {root vegetables}:  conf = 0.31, sup = 0.023\n",
            "{root vegetables} ---> {whole milk, other vegetables}:  conf = 0.213, sup = 0.023\n",
            "{other vegetables} ---> {whole milk, root vegetables}:  conf = 0.12, sup = 0.023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2"
      ],
      "metadata": {
        "id": "Ouow5B51hFzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "xpoints = H\n",
        "\n",
        "plt.plot(xpoints)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EAdRWloeg_lp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "01803bf6-601f-41b4-ff55-f0b66e2fd577"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-02fb6ccb8f5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mxpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2761\u001b[0m     return gca().plot(\n\u001b[1;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2763\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \"\"\"\n\u001b[1;32m   1646\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mupdate_units\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1514\u001b[0m         \u001b[0mneednew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverter\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1515\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1516\u001b[0;31m         \u001b[0mdefault\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1517\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/category.py\u001b[0m in \u001b[0;36mdefault_units\u001b[0;34m(data, axis)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# the conversion call stack is default_units -> axis_info -> convert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUnitData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/category.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/category.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m# check if convertible to number:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mconvertible\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m             \u001b[0;31m# OrderedDict just iterates over unique values in data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_isinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tWoPSvprcAFt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "M9Qk-ILicAw9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YJfpmgY9cA_m"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rp80NTxRcBQE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KecUxSsicBeR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 - FP Growth"
      ],
      "metadata": {
        "id": "KIwEgm5nz6n0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "FP-growth (\"frequent pattern growth\") is an algorithm for frequent item set mining and association rule learning over transactional databases.\n",
        "\n",
        "In the first pass, the algorithm counts occurrence of items (attribute-value pairs) in the dataset, and stores them to 'header table'. In the second pass, it builds the FP-tree structure by inserting instances. Items in each instance have to be sorted by descending order of their frequency in the dataset, so that the tree can be processed quickly. Items in each instance that do not meet minimum coverage threshold are discarded. If many instances share most frequent items, FP-tree provides high compression close to tree root.\n",
        "\n",
        "Recursive processing of this compressed version of main dataset grows large item sets directly, instead of generating candidate items and testing them against the entire database. Growth starts from the bottom of the header table (having longest branches), by finding all instances matching given condition. New tree is created, with counts projected from the original tree corresponding to the set of instances that are conditional on the attribute, with each node getting sum of its children counts. Recursive growth ends when no individual items conditional on the attribute meet minimum support threshold, and processing continues on the remaining header items of the original FP-tree."
      ],
      "metadata": {
        "id": "WT46qU381VcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (c) 2014 Reid Johnson\n",
        "#\n",
        "# Modified from:\n",
        "# Eric Naeseth <eric@naeseth.com>\n",
        "# (https://github.com/enaeseth/python-fp-growth/blob/master/fp_growth.py)\n",
        "#\n",
        "# A Python implementation of the FP-growth algorithm.\n",
        "\n",
        "from collections import defaultdict, namedtuple\n",
        "#from itertools import imap\n",
        "\n",
        "__author__ = 'Eric Naeseth <eric@naeseth.com>'\n",
        "__copyright__ = 'Copyright © 2009 Eric Naeseth'\n",
        "__license__ = 'MIT License'\n",
        "\n",
        "def fpgrowth(dataset, min_support=0.5, include_support=True, verbose=False):\n",
        "    \"\"\"Implements the FP-growth algorithm.\n",
        "\n",
        "    The `dataset` parameter can be any iterable of iterables of items.\n",
        "    `min_support` should be an integer specifying the minimum number of\n",
        "    occurrences of an itemset for it to be accepted.\n",
        "\n",
        "    Each item must be hashable (i.e., it must be valid as a member of a\n",
        "    dictionary or a set).\n",
        "\n",
        "    If `include_support` is true, yield (itemset, support) pairs instead of\n",
        "    just the itemsets.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset : list\n",
        "        The dataset (a list of transactions) from which to generate \n",
        "        candidate itemsets.\n",
        "\n",
        "    min_support : float\n",
        "        The minimum support threshold. Defaults to 0.5.\n",
        "\n",
        "    include_support : bool\n",
        "        Include support in output (default=False).\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] J. Han, J. Pei, Y. Yin, \"Mining Frequent Patterns without Candidate \n",
        "           Generation,\" 2000.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    F = []\n",
        "    support_data = {}\n",
        "    for k,v in find_frequent_itemsets(dataset, min_support=min_support, include_support=include_support, verbose=verbose):\n",
        "        F.append(frozenset(k))\n",
        "        support_data[frozenset(k)] = v\n",
        "\n",
        "    # Create one array with subarrays that hold all transactions of equal length.\n",
        "    def bucket_list(nested_list, sort=True):\n",
        "        bucket = defaultdict(list)\n",
        "        for sublist in nested_list:\n",
        "            bucket[len(sublist)].append(sublist)\n",
        "        return [v for k,v in sorted(bucket.items())] if sort else bucket.values()\n",
        "\n",
        "    F = bucket_list(F)\n",
        "    \n",
        "    return F, support_data\n",
        "\n",
        "def find_frequent_itemsets(dataset, min_support, include_support=False, verbose=False):\n",
        "    \"\"\"\n",
        "    Find frequent itemsets in the given transactions using FP-growth. This\n",
        "    function returns a generator instead of an eagerly-populated list of items.\n",
        "\n",
        "    The `dataset` parameter can be any iterable of iterables of items.\n",
        "    `min_support` should be an integer specifying the minimum number of\n",
        "    occurrences of an itemset for it to be accepted.\n",
        "\n",
        "    Each item must be hashable (i.e., it must be valid as a member of a\n",
        "    dictionary or a set).\n",
        "\n",
        "    If `include_support` is true, yield (itemset, support) pairs instead of\n",
        "    just the itemsets.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset : list\n",
        "        The dataset (a list of transactions) from which to generate \n",
        "        candidate itemsets.\n",
        "\n",
        "    min_support : float\n",
        "        The minimum support threshold. Defaults to 0.5.\n",
        "\n",
        "    include_support : bool\n",
        "        Include support in output (default=False).\n",
        "\n",
        "    \"\"\"\n",
        "    items = defaultdict(lambda: 0) # mapping from items to their supports\n",
        "    processed_transactions = []\n",
        "\n",
        "    # Load the passed-in transactions and count the support that individual\n",
        "    # items have.\n",
        "    for transaction in dataset:\n",
        "        processed = []\n",
        "        for item in transaction:\n",
        "            items[item] += 1\n",
        "            processed.append(item)\n",
        "        processed_transactions.append(processed)\n",
        "\n",
        "    # Remove infrequent items from the item support dictionary.\n",
        "    items = dict((item, support) for item, support in items.items()\n",
        "        if support >= min_support)\n",
        "\n",
        "    # Build our FP-tree. Before any transactions can be added to the tree, they\n",
        "    # must be stripped of infrequent items and their surviving items must be\n",
        "    # sorted in decreasing order of frequency.\n",
        "    def clean_transaction(transaction):\n",
        "        #transaction = filter(lambda v: v in items, transaction)\n",
        "        transaction.sort(key=lambda v: items[v], reverse=True)\n",
        "        return transaction\n",
        "\n",
        "    master = FPTree()\n",
        "    for transaction in map(clean_transaction, processed_transactions):\n",
        "        master.add(transaction)\n",
        "\n",
        "    support_data = {}\n",
        "    def find_with_suffix(tree, suffix):\n",
        "        for item, nodes in tree.items():\n",
        "            support = float(sum(n.count for n in nodes)) / len(dataset)\n",
        "            if support >= min_support and item not in suffix:\n",
        "                # New winner!\n",
        "                found_set = [item] + suffix\n",
        "                support_data[frozenset(found_set)] = support\n",
        "                yield (found_set, support) if include_support else found_set\n",
        "\n",
        "                # Build a conditional tree and recursively search for frequent\n",
        "                # itemsets within it.\n",
        "                cond_tree = conditional_tree_from_paths(tree.prefix_paths(item),\n",
        "                    min_support)\n",
        "                for s in find_with_suffix(cond_tree, found_set):\n",
        "                    yield s # pass along the good news to our caller\n",
        "\n",
        "    if verbose:\n",
        "        # Print a list of all the frequent itemsets.\n",
        "        for itemset, support in find_with_suffix(master, []):\n",
        "            print(\"\" \\\n",
        "                + \"{\" \\\n",
        "                + \"\".join(str(i) + \", \" for i in iter(itemset)).rstrip(', ') \\\n",
        "                + \"}\" \\\n",
        "                + \":  sup = \" + str(round(support_data[frozenset(itemset)], 3)))\n",
        "\n",
        "    # Search for frequent itemsets, and yield the results we find.\n",
        "    for itemset in find_with_suffix(master, []):\n",
        "        yield itemset\n",
        "\n",
        "class FPTree(object):\n",
        "    \"\"\"\n",
        "    An FP tree.\n",
        "\n",
        "    This object may only store transaction items that are hashable (i.e., all\n",
        "    items must be valid as dictionary keys or set members).\n",
        "    \"\"\"\n",
        "\n",
        "    Route = namedtuple('Route', 'head tail')\n",
        "\n",
        "    def __init__(self):\n",
        "        # The root node of the tree.\n",
        "        self._root = FPNode(self, None, None)\n",
        "\n",
        "        # A dictionary mapping items to the head and tail of a path of\n",
        "        # \"neighbors\" that will hit every node containing that item.\n",
        "        self._routes = {}\n",
        "\n",
        "    @property\n",
        "    def root(self):\n",
        "        \"\"\"The root node of the tree.\"\"\"\n",
        "        return self._root\n",
        "\n",
        "    def add(self, transaction):\n",
        "        \"\"\"\n",
        "        Adds a transaction to the tree.\n",
        "        \"\"\"\n",
        "\n",
        "        point = self._root\n",
        "\n",
        "        for item in transaction:\n",
        "            next_point = point.search(item)\n",
        "            if next_point:\n",
        "                # There is already a node in this tree for the current\n",
        "                # transaction item; reuse it.\n",
        "                next_point.increment()\n",
        "            else:\n",
        "                # Create a new point and add it as a child of the point we're\n",
        "                # currently looking at.\n",
        "                next_point = FPNode(self, item)\n",
        "                point.add(next_point)\n",
        "\n",
        "                # Update the route of nodes that contain this item to include\n",
        "                # our new node.\n",
        "                self._update_route(next_point)\n",
        "\n",
        "            point = next_point\n",
        "\n",
        "    def _update_route(self, point):\n",
        "        \"\"\"Add the given node to the route through all nodes for its item.\"\"\"\n",
        "        assert self is point.tree\n",
        "\n",
        "        try:\n",
        "            route = self._routes[point.item]\n",
        "            route[1].neighbor = point # route[1] is the tail\n",
        "            self._routes[point.item] = self.Route(route[0], point)\n",
        "        except KeyError:\n",
        "            # First node for this item; start a new route.\n",
        "            self._routes[point.item] = self.Route(point, point)\n",
        "\n",
        "    def items(self):\n",
        "        \"\"\"\n",
        "        Generate one 2-tuples for each item represented in the tree. The first\n",
        "        element of the tuple is the item itself, and the second element is a\n",
        "        generator that will yield the nodes in the tree that belong to the item.\n",
        "        \"\"\"\n",
        "        for item in self._routes.keys():\n",
        "            yield (item, self.nodes(item))\n",
        "\n",
        "            \n",
        "    def nodes(self, item):\n",
        "        \"\"\"\n",
        "        Generates the sequence of nodes that contain the given item.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            node = self._routes[item][0]\n",
        "        except KeyError:\n",
        "            return\n",
        "\n",
        "        while node:\n",
        "            yield node\n",
        "            node = node.neighbor\n",
        "\n",
        "    def prefix_paths(self, item):\n",
        "        \"\"\"Generates the prefix paths that end with the given item.\"\"\"\n",
        "\n",
        "        def collect_path(node):\n",
        "            path = []\n",
        "            while node and not node.root:\n",
        "                path.append(node)\n",
        "                node = node.parent\n",
        "            path.reverse()\n",
        "            return path\n",
        "\n",
        "        return (collect_path(node) for node in self.nodes(item))\n",
        "\n",
        "    def inspect(self):\n",
        "        print(\"Tree:\")\n",
        "        self.root.inspect(1)\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Routes:\")\n",
        "        for item, nodes in self.items():\n",
        "            print(\"  %r\" % item)\n",
        "            for node in nodes:\n",
        "                print(\"    %r\" % node)\n",
        "\n",
        "    def _removed(self, node):\n",
        "        \"\"\"Called when `node` is removed from the tree; performs cleanup.\"\"\"\n",
        "\n",
        "        head, tail = self._routes[node.item]\n",
        "        if node is head:\n",
        "            if node is tail or not node.neighbor:\n",
        "                # It was the sole node.\n",
        "                del self._routes[node.item]\n",
        "            else:\n",
        "                self._routes[node.item] = self.Route(node.neighbor, tail)\n",
        "        else:\n",
        "            for n in self.nodes(node.item):\n",
        "                if n.neighbor is node:\n",
        "                    n.neighbor = node.neighbor # skip over\n",
        "                    if node is tail:\n",
        "                        self._routes[node.item] = self.Route(head, n)\n",
        "                    break\n",
        "\n",
        "def conditional_tree_from_paths(paths, min_support):\n",
        "    \"\"\"Builds a conditional FP-tree from the given prefix paths.\"\"\"\n",
        "    tree = FPTree()\n",
        "    condition_item = None\n",
        "    items = set()\n",
        "\n",
        "    # Import the nodes in the paths into the new tree. Only the counts of the\n",
        "    # leaf notes matter; the remaining counts will be reconstructed from the\n",
        "    # leaf counts.\n",
        "    for path in paths:\n",
        "        if condition_item is None:\n",
        "            condition_item = path[-1].item\n",
        "\n",
        "        point = tree.root\n",
        "        for node in path:\n",
        "            next_point = point.search(node.item)\n",
        "            if not next_point:\n",
        "                # Add a new node to the tree.\n",
        "                items.add(node.item)\n",
        "                count = node.count if node.item == condition_item else 0\n",
        "                next_point = FPNode(tree, node.item, count)\n",
        "                point.add(next_point)\n",
        "                tree._update_route(next_point)\n",
        "            point = next_point\n",
        "\n",
        "    assert condition_item is not None\n",
        "\n",
        "    # Calculate the counts of the non-leaf nodes.\n",
        "    for path in tree.prefix_paths(condition_item):\n",
        "        count = path[-1].count\n",
        "        for node in reversed(path[:-1]):\n",
        "            node._count += count\n",
        "\n",
        "    # Eliminate the nodes for any items that are no longer frequent.\n",
        "    for item in items:\n",
        "        support = sum(n.count for n in tree.nodes(item))\n",
        "        if support < min_support:\n",
        "            # Doesn't make the cut anymore\n",
        "            for node in tree.nodes(item):\n",
        "                if node.parent is not None:\n",
        "                    node.parent.remove(node)\n",
        "\n",
        "    # Finally, remove the nodes corresponding to the item for which this\n",
        "    # conditional tree was generated.\n",
        "    for node in tree.nodes(condition_item):\n",
        "        if node.parent is not None: # the node might already be an orphan\n",
        "            node.parent.remove(node)\n",
        "\n",
        "    return tree\n",
        "\n",
        "class FPNode(object):\n",
        "    \"\"\"A node in an FP tree.\"\"\"\n",
        "\n",
        "    def __init__(self, tree, item, count=1):\n",
        "        self._tree = tree\n",
        "        self._item = item\n",
        "        self._count = count\n",
        "        self._parent = None\n",
        "        self._children = {}\n",
        "        self._neighbor = None\n",
        "\n",
        "    def add(self, child):\n",
        "        \"\"\"Adds the given FPNode `child` as a child of this node.\"\"\"\n",
        "\n",
        "        if not isinstance(child, FPNode):\n",
        "            raise TypeError(\"Can only add other FPNodes as children\")\n",
        "\n",
        "        if not child.item in self._children:\n",
        "            self._children[child.item] = child\n",
        "            child.parent = self\n",
        "\n",
        "    def search(self, item):\n",
        "        \"\"\"\n",
        "        Checks to see if this node contains a child node for the given item.\n",
        "        If so, that node is returned; otherwise, `None` is returned.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            return self._children[item]\n",
        "        except KeyError:\n",
        "            return None\n",
        "\n",
        "    def remove(self, child):\n",
        "        try:\n",
        "            if self._children[child.item] is child:\n",
        "                del self._children[child.item]\n",
        "                child.parent = None\n",
        "                self._tree._removed(child)\n",
        "                for sub_child in child.children:\n",
        "                    try:\n",
        "                        # Merger case: we already have a child for that item, so\n",
        "                        # add the sub-child's count to our child's count.\n",
        "                        self._children[sub_child.item]._count += sub_child.count\n",
        "                        sub_child.parent = None # it's an orphan now\n",
        "                    except KeyError:\n",
        "                        # Turns out we don't actually have a child, so just add\n",
        "                        # the sub-child as our own child.\n",
        "                        self.add(sub_child)\n",
        "                child._children = {}\n",
        "            else:\n",
        "                raise ValueError(\"that node is not a child of this node\")\n",
        "        except KeyError:\n",
        "            raise ValueError(\"that node is not a child of this node\")\n",
        "\n",
        "    def __contains__(self, item):\n",
        "        return item in self._children\n",
        "\n",
        "    @property\n",
        "    def tree(self):\n",
        "        \"\"\"The tree in which this node appears.\"\"\"\n",
        "        return self._tree\n",
        "\n",
        "    @property\n",
        "    def item(self):\n",
        "        \"\"\"The item contained in this node.\"\"\"\n",
        "        return self._item\n",
        "\n",
        "    @property\n",
        "    def count(self):\n",
        "        \"\"\"The count associated with this node's item.\"\"\"\n",
        "        return self._count\n",
        "\n",
        "    def increment(self):\n",
        "        \"\"\"Increments the count associated with this node's item.\"\"\"\n",
        "        if self._count is None:\n",
        "            raise ValueError(\"Root nodes have no associated count.\")\n",
        "        self._count += 1\n",
        "\n",
        "    @property\n",
        "    def root(self):\n",
        "        \"\"\"True if this node is the root of a tree; false if otherwise.\"\"\"\n",
        "        return self._item is None and self._count is None\n",
        "\n",
        "    @property\n",
        "    def leaf(self):\n",
        "        \"\"\"True if this node is a leaf in the tree; false if otherwise.\"\"\"\n",
        "        return len(self._children) == 0\n",
        "\n",
        "    def parent():\n",
        "        doc = \"The node's parent.\"\n",
        "        def fget(self):\n",
        "            return self._parent\n",
        "        def fset(self, value):\n",
        "            if value is not None and not isinstance(value, FPNode):\n",
        "                raise TypeError(\"A node must have an FPNode as a parent.\")\n",
        "            if value and value.tree is not self.tree:\n",
        "                raise ValueError(\"Cannot have a parent from another tree.\")\n",
        "            self._parent = value\n",
        "        return locals()\n",
        "    parent = property(**parent())\n",
        "\n",
        "    def neighbor():\n",
        "        doc = \"\"\"\n",
        "        The node's neighbor; the one with the same value that is \"to the right\"\n",
        "        of it in the tree.\n",
        "        \"\"\"\n",
        "        def fget(self):\n",
        "            return self._neighbor\n",
        "        def fset(self, value):\n",
        "            if value is not None and not isinstance(value, FPNode):\n",
        "                raise TypeError(\"A node must have an FPNode as a neighbor.\")\n",
        "            if value and value.tree is not self.tree:\n",
        "                raise ValueError(\"Cannot have a neighbor from another tree.\")\n",
        "            self._neighbor = value\n",
        "        return locals()\n",
        "    neighbor = property(**neighbor())\n",
        "\n",
        "    @property\n",
        "    def children(self):\n",
        "        \"\"\"The nodes that are children of this node.\"\"\"\n",
        "        return tuple(self._children.values())\n",
        "        \n",
        "    def inspect(self, depth=0):\n",
        "        print(('  ' * depth) + repr(self))\n",
        "        for child in self.children:\n",
        "            child.inspect(depth + 1)\n",
        "\n",
        "    def __repr__(self):\n",
        "        if self.root:\n",
        "            return \"<%s (root)>\" % type(self).__name__\n",
        "        return \"<%s %r (%r)>\" % (type(self).__name__, self.item, self.count)\n",
        "\n",
        "def rules_from_conseq(freq_set, H, support_data, rules, min_confidence=0.5, verbose=False):\n",
        "    \"\"\"Generates a set of candidate rules.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    freq_set : frozenset\n",
        "        The complete list of frequent itemsets.\n",
        "\n",
        "    H : list\n",
        "        A list of frequent itemsets (of a particular length).\n",
        "\n",
        "    support_data : dict\n",
        "        The support data for all candidate itemsets.\n",
        "\n",
        "    rules : list\n",
        "        A potentially incomplete set of candidate rules above the minimum \n",
        "        confidence threshold.\n",
        "\n",
        "    min_confidence : float\n",
        "        The minimum confidence threshold. Defaults to 0.5.\n",
        "    \"\"\"\n",
        "    m = len(H[0])\n",
        "    if m == 1:\n",
        "        Hmp1 = calc_confidence(freq_set, H, support_data, rules, min_confidence, verbose)\n",
        "    if (len(freq_set) > (m+1)):\n",
        "        Hmp1 = apriori_gen(H, m+1) # generate candidate itemsets\n",
        "        Hmp1 = calc_confidence(freq_set, Hmp1,  support_data, rules, min_confidence, verbose)\n",
        "        if len(Hmp1) > 1:\n",
        "            # If there are candidate rules above the minimum confidence \n",
        "            # threshold, recurse on the list of these candidate rules.\n",
        "            rules_from_conseq(freq_set, Hmp1, support_data, rules, min_confidence, verbose)\n",
        "\n",
        "def calc_confidence(freq_set, H, support_data, rules, min_confidence=0.5, verbose=False):\n",
        "    \"\"\"Evaluates the generated rules.\n",
        "\n",
        "    One measurement for quantifying the goodness of association rules is \n",
        "    confidence. The confidence for a rule 'P implies H' (P -> H) is defined as \n",
        "    the support for P and H divided by the support for P \n",
        "    (support (P|H) / support(P)), where the | symbol denotes the set union \n",
        "    (thus P|H means all the items in set P or in set H).\n",
        "\n",
        "    To calculate the confidence, we iterate through the frequent itemsets and \n",
        "    associated support data. For each frequent itemset, we divide the support \n",
        "    of the itemset by the support of the antecedent (left-hand-side of the \n",
        "    rule).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    freq_set : frozenset\n",
        "        The complete list of frequent itemsets.\n",
        "\n",
        "    H : list\n",
        "        A list of frequent itemsets (of a particular length).\n",
        "\n",
        "    min_support : float\n",
        "        The minimum support threshold.\n",
        "\n",
        "    rules : list\n",
        "        A potentially incomplete set of candidate rules above the minimum \n",
        "        confidence threshold.\n",
        "\n",
        "    min_confidence : float\n",
        "        The minimum confidence threshold. Defaults to 0.5.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pruned_H : list\n",
        "        The list of candidate rules above the minimum confidence threshold.\n",
        "    \"\"\"\n",
        "    pruned_H = [] # list of candidate rules above the minimum confidence threshold\n",
        "    for conseq in H: # iterate over the frequent itemsets\n",
        "        conf = support_data[freq_set] / support_data[freq_set - conseq]\n",
        "        if conf >= min_confidence:\n",
        "            rules.append((freq_set - conseq, conseq, conf))\n",
        "            pruned_H.append(conseq)\n",
        "\n",
        "            if verbose:\n",
        "                print(\"\" \\\n",
        "                    + \"{\" \\\n",
        "                    + \"\".join([str(i) + \", \" for i in iter(freq_set-conseq)]).rstrip(', ') \\\n",
        "                    + \"}\" \\\n",
        "                    + \" ---> \" \\\n",
        "                    + \"{\" \\\n",
        "                    + \"\".join([str(i) + \", \" for i in iter(conseq)]).rstrip(', ') \\\n",
        "                    + \"}\" \\\n",
        "                    + \":  conf = \" + str(round(conf, 3)) \\\n",
        "                    + \", sup = \" + str(round(support_data[freq_set], 3)))\n",
        "\n",
        "    return pruned_H\n",
        "\n",
        "def generate_rules(F, support_data, min_confidence=0.5, verbose=True):\n",
        "    \"\"\"Generates a set of candidate rules from a list of frequent itemsets.\n",
        "\n",
        "    For each frequent itemset, we calculate the confidence of using a\n",
        "    particular item as the rule consequent (right-hand-side of the rule). By \n",
        "    testing and merging the remaining rules, we recursively create a list of \n",
        "    pruned rules.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    F : list\n",
        "        A list of frequent itemsets.\n",
        "\n",
        "    support_data : dict\n",
        "        The corresponding support data for the frequent itemsets (L).\n",
        "\n",
        "    min_confidence : float\n",
        "        The minimum confidence threshold. Defaults to 0.5.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    rules : list\n",
        "        The list of candidate rules above the minimum confidence threshold.\n",
        "    \"\"\"\n",
        "    rules = []\n",
        "    for i in range(1, len(F)):\n",
        "        for freq_set in F[i]:\n",
        "            H1 = [frozenset([item]) for item in freq_set]\n",
        "            if (i > 1):\n",
        "                rules_from_conseq(freq_set, H1, support_data, rules, min_confidence, verbose)\n",
        "            else:\n",
        "                calc_confidence(freq_set, H1, support_data, rules, min_confidence, verbose)\n",
        "\n",
        "    return rules"
      ],
      "metadata": {
        "id": "a1alLukf1mh7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we load an example market basket transactions dataset (a list of lists), map it to a 'set' datatype (for programmatic reasons), and print the transactions. We import and use pprint to format the output."
      ],
      "metadata": {
        "id": "Nt-pbpus10U1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "\n",
        "def load_dataset():\n",
        "    \"\"\"Loads an example of market basket transactions for testing purposes.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A list (database) of lists (transactions). Each element of a transaction \n",
        "    is an item.\n",
        "    \"\"\"\n",
        "    return [['Bread', 'Milk'], \n",
        "            ['Bread', 'Diapers', 'Beer', 'Eggs'], \n",
        "            ['Milk', 'Diapers', 'Beer', 'Coke'], \n",
        "            ['Bread', 'Milk', 'Diapers', 'Beer'], \n",
        "            ['Bread', 'Milk', 'Diapers', 'Coke']]\n",
        "\n",
        "dataset = load_dataset() # list of transactions; each transaction is a list of items\n",
        "D = map(set, dataset) # set of transactions; each transaction is a list of items\n",
        "\n",
        "pprint.pprint(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dQ0Lkca1orb",
        "outputId": "52c2c3ac-1303-4d5f-d949-6caaf18437e4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Bread', 'Milk'],\n",
            " ['Bread', 'Diapers', 'Beer', 'Eggs'],\n",
            " ['Milk', 'Diapers', 'Beer', 'Coke'],\n",
            " ['Bread', 'Milk', 'Diapers', 'Beer'],\n",
            " ['Bread', 'Milk', 'Diapers', 'Coke']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we input the initial dataset into the 'fpgrowth' function (along with a minimum support threshold) and it will return a list of all the frequent itemsets:"
      ],
      "metadata": {
        "id": "fuIG-v0516sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate all the frequent itemsets using the FP-growth algorithm.\n",
        "F, support_data = fpgrowth(dataset, min_support=0.6, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM7rQ7jj15XL",
        "outputId": "4dd2c1ee-e945-429f-ff57-6e9e9d66698d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{Bread}:  sup = 0.8\n",
            "{Milk}:  sup = 0.8\n",
            "{Bread, Milk}:  sup = 0.6\n",
            "{Diapers}:  sup = 0.8\n",
            "{Bread, Diapers}:  sup = 0.6\n",
            "{Milk, Diapers}:  sup = 0.6\n",
            "{Beer}:  sup = 0.6\n",
            "{Diapers, Beer}:  sup = 0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a frequent itemset (here, extracted by the FP-growth algorithm), we can generate the association rules with high support and confidence (via the generate_rules function):"
      ],
      "metadata": {
        "id": "hz8GDtZ12AOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the association rules from a list of frequent itemsets.\n",
        "H = generate_rules(F, support_data, min_confidence=0.8, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qREDCI3Y193e",
        "outputId": "c0779c4d-a47d-4985-84f7-51d2738e7583"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{Beer} ---> {Diapers}:  conf = 1.0, sup = 0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 - Interest Factor"
      ],
      "metadata": {
        "id": "fZcfJrYi32ym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "F_fp , sd = fpgrowth(dataset, min_support = 0.04, verbose=True)\n",
        "H_fp = generate_rules(F_fp, sd, min_confidence= 0.3, verbose=True)"
      ],
      "metadata": {
        "id": "jMfQPRxX2ChY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d027c5a-c25e-4543-a97b-b0099145af39"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{Bread}:  sup = 0.8\n",
            "{Milk}:  sup = 0.8\n",
            "{Bread, Milk}:  sup = 0.6\n",
            "{Diapers}:  sup = 0.8\n",
            "{Bread, Diapers}:  sup = 0.6\n",
            "{Milk, Diapers}:  sup = 0.6\n",
            "{Bread, Milk, Diapers}:  sup = 0.4\n",
            "{Beer}:  sup = 0.6\n",
            "{Bread, Beer}:  sup = 0.4\n",
            "{Diapers, Beer}:  sup = 0.6\n",
            "{Bread, Diapers, Beer}:  sup = 0.4\n",
            "{Milk, Diapers, Beer}:  sup = 0.4\n",
            "{Bread, Milk, Diapers, Beer}:  sup = 0.2\n",
            "{Milk, Beer}:  sup = 0.4\n",
            "{Bread, Milk, Beer}:  sup = 0.2\n",
            "{Eggs}:  sup = 0.2\n",
            "{Bread, Eggs}:  sup = 0.2\n",
            "{Diapers, Eggs}:  sup = 0.2\n",
            "{Bread, Diapers, Eggs}:  sup = 0.2\n",
            "{Beer, Eggs}:  sup = 0.2\n",
            "{Bread, Beer, Eggs}:  sup = 0.2\n",
            "{Diapers, Beer, Eggs}:  sup = 0.2\n",
            "{Bread, Diapers, Beer, Eggs}:  sup = 0.2\n",
            "{Coke}:  sup = 0.4\n",
            "{Milk, Coke}:  sup = 0.4\n",
            "{Bread, Milk, Coke}:  sup = 0.2\n",
            "{Diapers, Coke}:  sup = 0.4\n",
            "{Milk, Diapers, Coke}:  sup = 0.4\n",
            "{Bread, Milk, Diapers, Coke}:  sup = 0.2\n",
            "{Bread, Diapers, Coke}:  sup = 0.2\n",
            "{Beer, Coke}:  sup = 0.2\n",
            "{Milk, Beer, Coke}:  sup = 0.2\n",
            "{Diapers, Beer, Coke}:  sup = 0.2\n",
            "{Milk, Diapers, Beer, Coke}:  sup = 0.2\n",
            "{Bread, Coke}:  sup = 0.2\n",
            "{Bread} ---> {Milk}:  conf = 0.75, sup = 0.6\n",
            "{Milk} ---> {Bread}:  conf = 0.75, sup = 0.6\n",
            "{Diapers} ---> {Bread}:  conf = 0.75, sup = 0.6\n",
            "{Bread} ---> {Diapers}:  conf = 0.75, sup = 0.6\n",
            "{Diapers} ---> {Milk}:  conf = 0.75, sup = 0.6\n",
            "{Milk} ---> {Diapers}:  conf = 0.75, sup = 0.6\n",
            "{Bread} ---> {Beer}:  conf = 0.5, sup = 0.4\n",
            "{Beer} ---> {Bread}:  conf = 0.667, sup = 0.4\n",
            "{Diapers} ---> {Beer}:  conf = 0.75, sup = 0.6\n",
            "{Beer} ---> {Diapers}:  conf = 1.0, sup = 0.6\n",
            "{Beer} ---> {Milk}:  conf = 0.667, sup = 0.4\n",
            "{Milk} ---> {Beer}:  conf = 0.5, sup = 0.4\n",
            "{Eggs} ---> {Bread}:  conf = 1.0, sup = 0.2\n",
            "{Eggs} ---> {Diapers}:  conf = 1.0, sup = 0.2\n",
            "{Beer} ---> {Eggs}:  conf = 0.333, sup = 0.2\n",
            "{Eggs} ---> {Beer}:  conf = 1.0, sup = 0.2\n",
            "{Coke} ---> {Milk}:  conf = 1.0, sup = 0.4\n",
            "{Milk} ---> {Coke}:  conf = 0.5, sup = 0.4\n",
            "{Diapers} ---> {Coke}:  conf = 0.5, sup = 0.4\n",
            "{Coke} ---> {Diapers}:  conf = 1.0, sup = 0.4\n",
            "{Coke} ---> {Beer}:  conf = 0.5, sup = 0.2\n",
            "{Beer} ---> {Coke}:  conf = 0.333, sup = 0.2\n",
            "{Coke} ---> {Bread}:  conf = 0.5, sup = 0.2\n",
            "{Bread, Diapers} ---> {Milk}:  conf = 0.667, sup = 0.4\n",
            "{Milk, Diapers} ---> {Bread}:  conf = 0.667, sup = 0.4\n",
            "{Milk, Bread} ---> {Diapers}:  conf = 0.667, sup = 0.4\n",
            "{Diapers} ---> {Milk, Bread}:  conf = 0.5, sup = 0.4\n",
            "{Bread} ---> {Milk, Diapers}:  conf = 0.5, sup = 0.4\n",
            "{Milk} ---> {Bread, Diapers}:  conf = 0.5, sup = 0.4\n",
            "{Bread, Diapers} ---> {Beer}:  conf = 0.667, sup = 0.4\n",
            "{Beer, Diapers} ---> {Bread}:  conf = 0.667, sup = 0.4\n",
            "{Beer, Bread} ---> {Diapers}:  conf = 1.0, sup = 0.4\n",
            "{Diapers} ---> {Beer, Bread}:  conf = 0.5, sup = 0.4\n",
            "{Bread} ---> {Beer, Diapers}:  conf = 0.5, sup = 0.4\n",
            "{Beer} ---> {Bread, Diapers}:  conf = 0.667, sup = 0.4\n",
            "{Beer, Diapers} ---> {Milk}:  conf = 0.667, sup = 0.4\n",
            "{Milk, Diapers} ---> {Beer}:  conf = 0.667, sup = 0.4\n",
            "{Milk, Beer} ---> {Diapers}:  conf = 1.0, sup = 0.4\n",
            "{Diapers} ---> {Milk, Beer}:  conf = 0.5, sup = 0.4\n",
            "{Beer} ---> {Milk, Diapers}:  conf = 0.667, sup = 0.4\n",
            "{Milk} ---> {Beer, Diapers}:  conf = 0.5, sup = 0.4\n",
            "{Beer, Bread} ---> {Milk}:  conf = 0.5, sup = 0.2\n",
            "{Milk, Bread} ---> {Beer}:  conf = 0.333, sup = 0.2\n",
            "{Milk, Beer} ---> {Bread}:  conf = 0.5, sup = 0.2\n",
            "{Beer} ---> {Milk, Bread}:  conf = 0.333, sup = 0.2\n",
            "{Bread, Diapers} ---> {Eggs}:  conf = 0.333, sup = 0.2\n",
            "{Eggs, Diapers} ---> {Bread}:  conf = 1.0, sup = 0.2\n",
            "{Eggs, Bread} ---> {Diapers}:  conf = 1.0, sup = 0.2\n",
            "{Eggs} ---> {Bread, Diapers}:  conf = 1.0, sup = 0.2\n",
            "{Beer, Bread} ---> {Eggs}:  conf = 0.5, sup = 0.2\n",
            "{Eggs, Bread} ---> {Beer}:  conf = 1.0, sup = 0.2\n",
            "{Eggs, Beer} ---> {Bread}:  conf = 1.0, sup = 0.2\n",
            "{Beer} ---> {Eggs, Bread}:  conf = 0.333, sup = 0.2\n",
            "{Eggs} ---> {Beer, Bread}:  conf = 1.0, sup = 0.2\n",
            "{Beer, Diapers} ---> {Eggs}:  conf = 0.333, sup = 0.2\n",
            "{Eggs, Diapers} ---> {Beer}:  conf = 1.0, sup = 0.2\n",
            "{Eggs, Beer} ---> {Diapers}:  conf = 1.0, sup = 0.2\n",
            "{Beer} ---> {Eggs, Diapers}:  conf = 0.333, sup = 0.2\n",
            "{Eggs} ---> {Beer, Diapers}:  conf = 1.0, sup = 0.2\n",
            "{Coke, Bread} ---> {Milk}:  conf = 1.0, sup = 0.2\n",
            "{Milk, Bread} ---> {Coke}:  conf = 0.333, sup = 0.2\n",
            "{Milk, Coke} ---> {Bread}:  conf = 0.5, sup = 0.2\n",
            "{Coke} ---> {Milk, Bread}:  conf = 0.5, sup = 0.2\n",
            "{Coke, Diapers} ---> {Milk}:  conf = 1.0, sup = 0.4\n",
            "{Milk, Diapers} ---> {Coke}:  conf = 0.667, sup = 0.4\n",
            "{Milk, Coke} ---> {Diapers}:  conf = 1.0, sup = 0.4\n",
            "{Diapers} ---> {Milk, Coke}:  conf = 0.5, sup = 0.4\n",
            "{Coke} ---> {Milk, Diapers}:  conf = 1.0, sup = 0.4\n",
            "{Milk} ---> {Coke, Diapers}:  conf = 0.5, sup = 0.4\n",
            "{Bread, Diapers} ---> {Coke}:  conf = 0.333, sup = 0.2\n",
            "{Coke, Diapers} ---> {Bread}:  conf = 0.5, sup = 0.2\n",
            "{Coke, Bread} ---> {Diapers}:  conf = 1.0, sup = 0.2\n",
            "{Coke} ---> {Bread, Diapers}:  conf = 0.5, sup = 0.2\n",
            "{Beer, Coke} ---> {Milk}:  conf = 1.0, sup = 0.2\n",
            "{Milk, Coke} ---> {Beer}:  conf = 0.5, sup = 0.2\n",
            "{Milk, Beer} ---> {Coke}:  conf = 0.5, sup = 0.2\n",
            "{Coke} ---> {Milk, Beer}:  conf = 0.5, sup = 0.2\n",
            "{Beer} ---> {Milk, Coke}:  conf = 0.333, sup = 0.2\n",
            "{Coke, Diapers} ---> {Beer}:  conf = 0.5, sup = 0.2\n",
            "{Beer, Diapers} ---> {Coke}:  conf = 0.333, sup = 0.2\n",
            "{Beer, Coke} ---> {Diapers}:  conf = 1.0, sup = 0.2\n",
            "{Coke} ---> {Beer, Diapers}:  conf = 0.5, sup = 0.2\n",
            "{Beer} ---> {Coke, Diapers}:  conf = 0.333, sup = 0.2\n",
            "{Beer, Bread, Diapers} ---> {Milk}:  conf = 0.5, sup = 0.2\n",
            "{Milk, Bread, Diapers} ---> {Beer}:  conf = 0.5, sup = 0.2\n",
            "{Milk, Beer, Diapers} ---> {Bread}:  conf = 0.5, sup = 0.2\n",
            "{Milk, Beer, Bread} ---> {Diapers}:  conf = 1.0, sup = 0.2\n",
            "{Bread, Diapers} ---> {Milk, Beer}:  conf = 0.333, sup = 0.2\n",
            "{Beer, Diapers} ---> {Milk, Bread}:  conf = 0.333, sup = 0.2\n",
            "{Beer, Bread} ---> {Milk, Diapers}:  conf = 0.5, sup = 0.2\n",
            "{Milk, Diapers} ---> {Beer, Bread}:  conf = 0.333, sup = 0.2\n",
            "{Milk, Bread} ---> {Beer, Diapers}:  conf = 0.333, sup = 0.2\n",
            "{Milk, Beer} ---> {Bread, Diapers}:  conf = 0.5, sup = 0.2\n",
            "{Beer} ---> {Milk, Bread, Diapers}:  conf = 0.333, sup = 0.2\n",
            "{Beer, Bread, Diapers} ---> {Eggs}:  conf = 0.5, sup = 0.2\n",
            "{Eggs, Bread, Diapers} ---> {Beer}:  conf = 1.0, sup = 0.2\n",
            "{Eggs, Beer, Diapers} ---> {Bread}:  conf = 1.0, sup = 0.2\n",
            "{Eggs, Beer, Bread} ---> {Diapers}:  conf = 1.0, sup = 0.2\n",
            "{Bread, Diapers} ---> {Eggs, Beer}:  conf = 0.333, sup = 0.2\n",
            "{Beer, Diapers} ---> {Eggs, Bread}:  conf = 0.333, sup = 0.2\n",
            "{Beer, Bread} ---> {Eggs, Diapers}:  conf = 0.5, sup = 0.2\n",
            "{Eggs, Diapers} ---> {Beer, Bread}:  conf = 1.0, sup = 0.2\n",
            "{Eggs, Bread} ---> {Beer, Diapers}:  conf = 1.0, sup = 0.2\n",
            "{Eggs, Beer} ---> {Bread, Diapers}:  conf = 1.0, sup = 0.2\n",
            "{Beer} ---> {Eggs, Bread, Diapers}:  conf = 0.333, sup = 0.2\n",
            "{Eggs} ---> {Beer, Bread, Diapers}:  conf = 1.0, sup = 0.2\n",
            "{Coke, Bread, Diapers} ---> {Milk}:  conf = 1.0, sup = 0.2\n",
            "{Milk, Bread, Diapers} ---> {Coke}:  conf = 0.5, sup = 0.2\n",
            "{Milk, Coke, Diapers} ---> {Bread}:  conf = 0.5, sup = 0.2\n",
            "{Milk, Coke, Bread} ---> {Diapers}:  conf = 1.0, sup = 0.2\n",
            "{Bread, Diapers} ---> {Milk, Coke}:  conf = 0.333, sup = 0.2\n",
            "{Coke, Diapers} ---> {Milk, Bread}:  conf = 0.5, sup = 0.2\n",
            "{Coke, Bread} ---> {Milk, Diapers}:  conf = 1.0, sup = 0.2\n",
            "{Milk, Diapers} ---> {Coke, Bread}:  conf = 0.333, sup = 0.2\n",
            "{Milk, Bread} ---> {Coke, Diapers}:  conf = 0.333, sup = 0.2\n",
            "{Milk, Coke} ---> {Bread, Diapers}:  conf = 0.5, sup = 0.2\n",
            "{Coke} ---> {Milk, Bread, Diapers}:  conf = 0.5, sup = 0.2\n",
            "{Beer, Coke, Diapers} ---> {Milk}:  conf = 1.0, sup = 0.2\n",
            "{Milk, Coke, Diapers} ---> {Beer}:  conf = 0.5, sup = 0.2\n",
            "{Milk, Beer, Diapers} ---> {Coke}:  conf = 0.5, sup = 0.2\n",
            "{Milk, Beer, Coke} ---> {Diapers}:  conf = 1.0, sup = 0.2\n",
            "{Coke, Diapers} ---> {Milk, Beer}:  conf = 0.5, sup = 0.2\n",
            "{Beer, Diapers} ---> {Milk, Coke}:  conf = 0.333, sup = 0.2\n",
            "{Beer, Coke} ---> {Milk, Diapers}:  conf = 1.0, sup = 0.2\n",
            "{Milk, Diapers} ---> {Beer, Coke}:  conf = 0.333, sup = 0.2\n",
            "{Milk, Coke} ---> {Beer, Diapers}:  conf = 0.5, sup = 0.2\n",
            "{Milk, Beer} ---> {Coke, Diapers}:  conf = 0.5, sup = 0.2\n",
            "{Coke} ---> {Milk, Beer, Diapers}:  conf = 0.5, sup = 0.2\n",
            "{Beer} ---> {Milk, Coke, Diapers}:  conf = 0.333, sup = 0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#IF\n",
        "data = []\n",
        "rules = []\n",
        "support = []\n",
        "i_f = {}\n",
        "\n",
        "for i in range(len(H_fp)):\n",
        "    sA=sd[H_fp[i][0]]\n",
        "    sB=sd[H_fp[i][1]]\n",
        "    temp1,temp2=\"\",\"\"\n",
        "    for item1 in H_fp[i][0]:\n",
        "        temp1 = item1\n",
        "    for item2 in H_fp[i][1]:\n",
        "        temp2 = item2\n",
        "    t=frozenset([temp1,temp2])\n",
        "    sAB=sd[t]\n",
        "    inf=sAB/(sA*sB)\n",
        "    item=temp1+\"-->\"+temp2\n",
        "    rules.append([item,sAB,H_fp[i][2],inf])\n",
        "    i_f[item] = inf\n",
        "print(i_f)"
      ],
      "metadata": {
        "id": "1oqLgK1I28cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "604f8957-2955-4c67-fe3d-29a94e0b04b0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Bread-->Milk': 3.749999999999999, 'Milk-->Bread': 0.9374999999999998, 'Diapers-->Bread': 5.0, 'Bread-->Diapers': 2.5, 'Diapers-->Milk': 3.749999999999999, 'Milk-->Diapers': 1.8749999999999996, 'Bread-->Beer': 3.3333333333333335, 'Beer-->Bread': 3.3333333333333335, 'Diapers-->Beer': 3.749999999999999, 'Beer-->Diapers': 2.5, 'Beer-->Milk': 0.8333333333333334, 'Milk-->Beer': 0.8333333333333334, 'Eggs-->Bread': 2.4999999999999996, 'Eggs-->Diapers': 2.4999999999999996, 'Beer-->Eggs': 1.6666666666666667, 'Eggs-->Beer': 1.6666666666666667, 'Coke-->Milk': 2.4999999999999996, 'Milk-->Coke': 1.2499999999999998, 'Diapers-->Coke': 3.3333333333333335, 'Coke-->Diapers': 2.4999999999999996, 'Coke-->Beer': 1.2499999999999998, 'Beer-->Coke': 0.8333333333333334, 'Coke-->Bread': 0.8333333333333334, 'Diapers-->Eggs': 2.4999999999999996, 'Bread-->Eggs': 2.4999999999999996, 'Bread-->Coke': 0.8333333333333334}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "names=[\"ITEM\",\"SUPPORT\",\"CONFIDENCE\",\"IF\"]\n",
        "df = pd.DataFrame(rules,columns=names)\n",
        "sup=df.sort_values(by=['SUPPORT'],ascending=True).head(5)\n",
        "print(sup)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UamTPzX9V9WU",
        "outputId": "018b76ad-2a4e-4267-a9ea-79825a897a44"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               ITEM  SUPPORT  CONFIDENCE        IF\n",
            "105  Eggs-->Diapers      0.2    1.000000  2.500000\n",
            "62     Coke-->Bread      0.2    0.500000  0.833333\n",
            "22     Coke-->Bread      0.2    0.500000  0.625000\n",
            "21      Beer-->Coke      0.2    0.333333  0.833333\n",
            "20      Coke-->Beer      0.2    0.500000  0.833333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conf=df.sort_values(by=['CONFIDENCE'],ascending=True).head(5)\n",
        "print(conf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmWRM_zaWh2S",
        "outputId": "0ccf011c-3a18-449a-dc0a-f29c1a3ce9ef"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               ITEM  SUPPORT  CONFIDENCE        IF\n",
            "128  Beer-->Diapers      0.6    0.333333  2.500000\n",
            "21      Beer-->Coke      0.2    0.333333  0.833333\n",
            "54   Diapers-->Eggs      0.2    0.333333  1.666667\n",
            "104  Beer-->Diapers      0.6    0.333333  5.000000\n",
            "82   Beer-->Diapers      0.6    0.333333  2.500000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inf=df.sort_values(by=['IF'],ascending=True).head(5)\n",
        "print(inf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLerSRrdWnjx",
        "outputId": "1165433f-6f7b-4665-a7b9-15055ea90654"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            ITEM  SUPPORT  CONFIDENCE        IF\n",
            "22  Coke-->Bread      0.2    0.500000  0.625000\n",
            "61  Coke-->Bread      0.2    0.500000  0.625000\n",
            "20   Coke-->Beer      0.2    0.500000  0.833333\n",
            "74   Coke-->Beer      0.2    0.500000  0.833333\n",
            "77   Beer-->Coke      0.2    0.333333  0.833333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dn0NlWcwWrda"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [conda env:python3]",
      "language": "python",
      "name": "conda-env-python3-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "DM - Assignment4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}